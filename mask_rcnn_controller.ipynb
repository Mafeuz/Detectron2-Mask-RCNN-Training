{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTV4jWZMYMlu"
   },
   "source": [
    "### Installing Detectron2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\r\n",
      "Copyright (C) 2019 Free Software Foundation, Inc.\r\n",
      "This is free software; see the source for copying conditions.  There is NO\r\n",
      "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# install dependencies: \n",
    "#!pip install pyyaml==5.1\n",
    "#!pip install onnx==1.8.0\n",
    "\n",
    "#!pip install pyyaml==5.1\n",
    "#!pip install torch==1.8.0+cu101 torchvision==0.9.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "\n",
    "#!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.8/index.html\n",
    "\n",
    "!gcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6pjRZdeiYPPI",
    "outputId": "16c8ff02-22db-4271-f0bf-091fbfcac119"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Ver: 1.8.0+cu101\n",
      "Torchvision: 0.9.0+cu101\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import cv2\n",
    "\n",
    "print('Torch Ver:', torch.__version__)\n",
    "print('Torchvision:', torchvision.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Dataset and Training MASK-RCNN Segmentation Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "NGt-e47XTgas"
   },
   "outputs": [],
   "source": [
    "from det2_mask_rcnn_train import det2_mask_build, det2_mask_train\n",
    "\n",
    "classes    = ['fringe']\n",
    "img_size   = (2000, 2000)\n",
    "data_path  = '/dataset/'\n",
    "batch_size = 2\n",
    "lr         = 0.00025\n",
    "workers    = 2\n",
    "max_iter   = 1000\n",
    "\n",
    "cfg, metadata = det2_mask_build(data_path, img_size, classes, batch_size, lr, workers, max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/11 15:19:29 d2.engine.defaults]: \u001b[0mModel:\n",
      "GeneralizedRCNN(\n",
      "  (backbone): FPN(\n",
      "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (top_block): LastLevelMaxPool()\n",
      "    (bottom_up): ResNet(\n",
      "      (stem): BasicStem(\n",
      "        (conv1): Conv2d(\n",
      "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (res2): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res3): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res4): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (4): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (5): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res5): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (proposal_generator): RPN(\n",
      "    (rpn_head): StandardRPNHead(\n",
      "      (conv): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (anchor_generator): DefaultAnchorGenerator(\n",
      "      (cell_anchors): BufferList()\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): StandardROIHeads(\n",
      "    (box_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (box_head): FastRCNNConvFCHead(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc_relu1): ReLU()\n",
      "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (fc_relu2): ReLU()\n",
      "    )\n",
      "    (box_predictor): FastRCNNOutputLayers(\n",
      "      (cls_score): Linear(in_features=1024, out_features=3, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n",
      "    )\n",
      "    (mask_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (mask_head): MaskRCNNConvUpsampleHead(\n",
      "      (mask_fcn1): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn2): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn3): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn4): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (deconv_relu): ReLU()\n",
      "      (predictor): Conv2d(256, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/11 15:19:30 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 102 images left.\n",
      "\u001b[32m[07/11 15:19:30 d2.data.build]: \u001b[0mDistribution of instances among all 1 categories:\n",
      "\u001b[36m|  category  | #instances   |\n",
      "|:----------:|:-------------|\n",
      "|   fringe   | 119          |\n",
      "|            |              |\u001b[0m\n",
      "\u001b[32m[07/11 15:19:30 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
      "\u001b[32m[07/11 15:19:30 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
      "\u001b[32m[07/11 15:19:30 d2.data.common]: \u001b[0mSerializing 102 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/11 15:19:30 d2.data.common]: \u001b[0mSerialized dataset takes 0.07 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/11 15:19:30 d2.solver.build]: \u001b[0mSOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (3, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (3,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (8, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (8,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.mask_head.predictor.weight' to the model due to incompatible shapes: (80, 256, 1, 1) in the checkpoint but (2, 256, 1, 1) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.mask_head.predictor.bias' to the model due to incompatible shapes: (80,) in the checkpoint but (2,) in the model! You might want to double check if this is expected.\n",
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mroi_heads.box_predictor.bbox_pred.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_predictor.cls_score.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.predictor.{bias, weight}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/11 15:19:30 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n",
      "\u001b[32m[07/11 15:19:33 d2.utils.events]: \u001b[0m eta: 0:02:25  iter: 19  total_loss: 1.802  loss_cls: 0.9332  loss_box_reg: 0.1603  loss_mask: 0.6879  loss_rpn_cls: 0.007463  loss_rpn_loc: 0.01248  time: 0.1486  data_time: 0.0201  lr: 4.9953e-06  max_mem: 1859M\n",
      "\u001b[32m[07/11 15:19:36 d2.utils.events]: \u001b[0m eta: 0:02:22  iter: 39  total_loss: 1.68  loss_cls: 0.8039  loss_box_reg: 0.168  loss_mask: 0.678  loss_rpn_cls: 0.007621  loss_rpn_loc: 0.01218  time: 0.1475  data_time: 0.0042  lr: 9.9902e-06  max_mem: 1859M\n",
      "\u001b[32m[07/11 15:19:39 d2.utils.events]: \u001b[0m eta: 0:02:19  iter: 59  total_loss: 1.379  loss_cls: 0.5438  loss_box_reg: 0.1506  loss_mask: 0.6412  loss_rpn_cls: 0.004554  loss_rpn_loc: 0.01164  time: 0.1483  data_time: 0.0041  lr: 1.4985e-05  max_mem: 1859M\n",
      "\u001b[32m[07/11 15:19:39 d2.engine.hooks]: \u001b[0mOverall training speed: 61 iterations in 0:00:09 (0.1486 s / it)\n",
      "\u001b[32m[07/11 15:19:39 d2.engine.hooks]: \u001b[0mTotal training time: 0:00:09 (0:00:00 on hooks)\n",
      "\u001b[32m[07/11 15:19:39 d2.utils.events]: \u001b[0m eta: 0:02:19  iter: 63  total_loss: 1.333  loss_cls: 0.534  loss_box_reg: 0.1549  loss_mask: 0.6358  loss_rpn_cls: 0.004943  loss_rpn_loc: 0.01136  time: 0.1484  data_time: 0.0041  lr: 1.5734e-05  max_mem: 1859M\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdet2_mask_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documentos/detectron_2_venv/det2_mask_rcnn_train.py:128\u001b[0m, in \u001b[0;36mdet2_mask_train\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m    126\u001b[0m trainer \u001b[38;5;241m=\u001b[39m DefaultTrainer(cfg) \n\u001b[1;32m    127\u001b[0m trainer\u001b[38;5;241m.\u001b[39mresume_or_load(resume\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 128\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documentos/detectron_2_venv/lib/python3.8/site-packages/detectron2/engine/defaults.py:484\u001b[0m, in \u001b[0;36mDefaultTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;124;03m    Run training.\u001b[39;00m\n\u001b[1;32m    480\u001b[0m \n\u001b[1;32m    481\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;124;03m        OrderedDict of results, if evaluation is enabled. Otherwise None.\u001b[39;00m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 484\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    485\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mTEST\u001b[38;5;241m.\u001b[39mEXPECTED_RESULTS) \u001b[38;5;129;01mand\u001b[39;00m comm\u001b[38;5;241m.\u001b[39mis_main_process():\n\u001b[1;32m    486\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\n\u001b[1;32m    487\u001b[0m             \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_last_eval_results\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    488\u001b[0m         ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo evaluation results obtained during training!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Documentos/detectron_2_venv/lib/python3.8/site-packages/detectron2/engine/train_loop.py:149\u001b[0m, in \u001b[0;36mTrainerBase.train\u001b[0;34m(self, start_iter, max_iter)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_iter, max_iter):\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbefore_step()\n\u001b[0;32m--> 149\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter_step()\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# self.iter == max_iter can be used by `after_train` to\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;66;03m# tell whether the training successfully finished or failed\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# due to exceptions.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documentos/detectron_2_venv/lib/python3.8/site-packages/detectron2/engine/defaults.py:494\u001b[0m, in \u001b[0;36mDefaultTrainer.run_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_step\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer\u001b[38;5;241m.\u001b[39miter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter\n\u001b[0;32m--> 494\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documentos/detectron_2_venv/lib/python3.8/site-packages/detectron2/engine/train_loop.py:273\u001b[0m, in \u001b[0;36mSimpleTrainer.run_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    268\u001b[0m data_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m start\n\u001b[1;32m    270\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;124;03mIf you want to do something with the losses, you can wrap the model.\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 273\u001b[0m loss_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(loss_dict, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    275\u001b[0m     losses \u001b[38;5;241m=\u001b[39m loss_dict\n",
      "File \u001b[0;32m~/Documentos/detectron_2_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    891\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    892\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    893\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/Documentos/detectron_2_venv/lib/python3.8/site-packages/detectron2/modeling/meta_arch/rcnn.py:154\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[0;34m(self, batched_inputs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     gt_instances \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 154\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproposal_generator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    157\u001b[0m     proposals, proposal_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproposal_generator(images, features, gt_instances)\n",
      "File \u001b[0;32m~/Documentos/detectron_2_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    891\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    892\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    893\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/Documentos/detectron_2_venv/lib/python3.8/site-packages/detectron2/modeling/backbone/fpn.py:126\u001b[0m, in \u001b[0;36mFPN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m        input (dict[str->Tensor]): mapping feature map name (e.g., \"res5\") to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03m            [\"p2\", \"p3\", ..., \"p6\"].\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m     bottom_up_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbottom_up\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m     results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    128\u001b[0m     prev_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlateral_convs[\u001b[38;5;241m0\u001b[39m](bottom_up_features[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_features[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\n",
      "File \u001b[0;32m~/Documentos/detectron_2_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    891\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    892\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    893\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/Documentos/detectron_2_venv/lib/python3.8/site-packages/detectron2/modeling/backbone/resnet.py:449\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    447\u001b[0m     outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstem\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, stage \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstage_names, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstages):\n\u001b[0;32m--> 449\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_out_features:\n\u001b[1;32m    451\u001b[0m         outputs[name] \u001b[38;5;241m=\u001b[39m x\n",
      "File \u001b[0;32m~/Documentos/detectron_2_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    891\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    892\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    893\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/Documentos/detectron_2_venv/lib/python3.8/site-packages/torch/nn/modules/container.py:119\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 119\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Documentos/detectron_2_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:890\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    889\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    891\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    892\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    893\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n\u001b[1;32m    894\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hook_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "det2_mask_train(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z4f1Tot2esU-"
   },
   "source": [
    "### Running Inference using the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mask_inference import load_maskrcnn, det2_maskrcnn_inference\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path   = 'fringe_segmentation.pth'\n",
    "classes_list = ['fringe']\n",
    "conf_thresh  = 0.5\n",
    "\n",
    "cfg, predictor, metadata = load_maskrcnn(model_path, classes_list, conf_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_path = '/home/autaza/Documentos/detectron_2_venv/paint_dataset/test/'\n",
    "imgs_list = glob.glob(imgs_path + '*.bmp')\n",
    "\n",
    "for i, img_path in enumerate(imgs_list):\n",
    "    \n",
    "    img_detection = det2_maskrcnn_inference(predictor, metadata, img_path, resize=(1280,1280), print_out=False, visualize=True, save_detection=False)\n",
    "\n",
    "    #print(f'IMG: {i} - Output:', img_detection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Model to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import detectron2\n",
    "import onnx\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.export import export_onnx_model, export_caffe2_model\n",
    "from detectron2.modeling import build_model\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "import detectron2.data.transforms as T\n",
    "\n",
    "# From Module:\n",
    "from mask_inference import load_maskrcnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onnx_export(model_path, classes_list, conf_thresh, img_size, save_name):\n",
    "    \n",
    "    '''\n",
    "        Method for converting .pt detectron2 mask-rcnn model to onnx\n",
    "    '''\n",
    "    \n",
    "    print('Loading Model...')\n",
    "    cfg, _, _ = load_maskrcnn(model_path, classes_list, conf_thresh)\n",
    "\n",
    "    model = build_model(cfg)\n",
    "    model.eval()\n",
    "    checkpointer = DetectionCheckpointer(model)\n",
    "    checkpointer.load(cfg.MODEL.WEIGHTS)\n",
    "    aug = T.ResizeShortestEdge([cfg.INPUT.MIN_SIZE_TEST, cfg.INPUT.MIN_SIZE_TEST],\n",
    "                               cfg.INPUT.MAX_SIZE_TEST)\n",
    "\n",
    "    print('Min Input:', cfg.INPUT.MIN_SIZE_TEST)\n",
    "    print('Max Input:', cfg.INPUT.MAX_SIZE_TEST)\n",
    "    \n",
    "    height, width, channels = img_size\n",
    "    \n",
    "    print('Collecting valid input img:')\n",
    "    !wget http://images.cocodataset.org/val2017/000000439715.jpg -q -O input.jpg\n",
    "    img = cv2.imread(\"./input.jpg\")\n",
    "    \n",
    "    image = aug.get_transform(img).apply_image(img)\n",
    "    image = torch.as_tensor(image.astype(\"float32\").transpose(2, 0, 1))\n",
    "    inputs = {\"image\": image, \"height\": height, \"width\": width}\n",
    "    \n",
    "    # Export to Onnx model\n",
    "    print('Exporting model to onnx...')\n",
    "    onnxModel = export_onnx_model(cfg, model, [inputs])\n",
    "    onnx.save(onnxModel, f'{save_name}.onnx')\n",
    "    print('Process completed, model onnx available!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model...\n",
      "Min Input: 800\n",
      "Max Input: 1333\n",
      "Collecting valid input img:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "export_caffe2_model() is deprecated. Please use `Caffe2Tracer().export_onnx() instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting model to onnx...\n",
      "WARNING: ONNX Optimizer has been moved to https://github.com/onnx/optimizer.\n",
      "All further enhancements and fixes to optimizers will be done in this new repo.\n",
      "The optimizer code in onnx/onnx repo will be removed in 1.9 release.\n",
      "\n",
      "Process completed, model onnx available!\n"
     ]
    }
   ],
   "source": [
    "model_path   = '/home/autaza/Documentos/detectron_2_venv/fringe_segmentation.pth'\n",
    "classes_list = ['fringe']\n",
    "conf_thresh  = 0.5\n",
    "img_size     = (2000, 2000, 3)\n",
    "save_name    = 'fringe_seg'\n",
    "\n",
    "onnx_export(model_path, classes_list, conf_thresh, img_size, save_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CFG Detectron2 Config:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "dDqreXOkZJIR"
   },
   "source": [
    "'''\n",
    "# -----------------------------------------------------------------------------\n",
    "# Convention about Training / Test specific parameters\n",
    "# -----------------------------------------------------------------------------\n",
    "# Whenever an argument can be either used for training or for testing, the\n",
    "# corresponding name will be post-fixed by a _TRAIN for a training parameter,\n",
    "# or _TEST for a test-specific parameter.\n",
    "# For example, the number of images during training will be\n",
    "# IMAGES_PER_BATCH_TRAIN, while the number of images for testing will be\n",
    "# IMAGES_PER_BATCH_TEST\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Config definition\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "_C = CN()\n",
    "\n",
    "# The version number, to upgrade from old configs to new ones if any\n",
    "# changes happen. It's recommended to keep a VERSION in your config file.\n",
    "_C.VERSION = 2\n",
    "\n",
    "_C.MODEL = CN()\n",
    "_C.MODEL.LOAD_PROPOSALS = False\n",
    "_C.MODEL.MASK_ON = False\n",
    "_C.MODEL.KEYPOINT_ON = False\n",
    "_C.MODEL.DEVICE = \"cuda\"\n",
    "_C.MODEL.META_ARCHITECTURE = \"GeneralizedRCNN\"\n",
    "\n",
    "# Path (a file path, or URL like detectron2://.., https://..) to a checkpoint file\n",
    "# to be loaded to the model. You can find available models in the model zoo.\n",
    "_C.MODEL.WEIGHTS = \"\"\n",
    "\n",
    "# Values to be used for image normalization (BGR order, since INPUT.FORMAT defaults to BGR).\n",
    "# To train on images of different number of channels, just set different mean & std.\n",
    "# Default values are the mean pixel value from ImageNet: [103.53, 116.28, 123.675]\n",
    "_C.MODEL.PIXEL_MEAN = [103.530, 116.280, 123.675]\n",
    "# When using pre-trained models in Detectron1 or any MSRA models,\n",
    "# std has been absorbed into its conv1 weights, so the std needs to be set 1.\n",
    "# Otherwise, you can use [57.375, 57.120, 58.395] (ImageNet std)\n",
    "_C.MODEL.PIXEL_STD = [1.0, 1.0, 1.0]\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# INPUT\n",
    "# -----------------------------------------------------------------------------\n",
    "_C.INPUT = CN()\n",
    "# By default, {MIN,MAX}_SIZE options are used in transforms.ResizeShortestEdge.\n",
    "# Please refer to ResizeShortestEdge for detailed definition.\n",
    "# Size of the smallest side of the image during training\n",
    "_C.INPUT.MIN_SIZE_TRAIN = (800,)\n",
    "# Sample size of smallest side by choice or random selection from range give by\n",
    "# INPUT.MIN_SIZE_TRAIN\n",
    "_C.INPUT.MIN_SIZE_TRAIN_SAMPLING = \"choice\"\n",
    "# Maximum size of the side of the image during training\n",
    "_C.INPUT.MAX_SIZE_TRAIN = 1333\n",
    "# Size of the smallest side of the image during testing. Set to zero to disable resize in testing.\n",
    "_C.INPUT.MIN_SIZE_TEST = 800\n",
    "# Maximum size of the side of the image during testing\n",
    "_C.INPUT.MAX_SIZE_TEST = 1333\n",
    "# Mode for flipping images used in data augmentation during training\n",
    "# choose one of [\"horizontal, \"vertical\", \"none\"]\n",
    "_C.INPUT.RANDOM_FLIP = \"horizontal\"\n",
    "\n",
    "# `True` if cropping is used for data augmentation during training\n",
    "_C.INPUT.CROP = CN({\"ENABLED\": False})\n",
    "# Cropping type. See documentation of `detectron2.data.transforms.RandomCrop` for explanation.\n",
    "_C.INPUT.CROP.TYPE = \"relative_range\"\n",
    "# Size of crop in range (0, 1] if CROP.TYPE is \"relative\" or \"relative_range\" and in number of\n",
    "# pixels if CROP.TYPE is \"absolute\"\n",
    "_C.INPUT.CROP.SIZE = [0.9, 0.9]\n",
    "\n",
    "\n",
    "# Whether the model needs RGB, YUV, HSV etc.\n",
    "# Should be one of the modes defined here, as we use PIL to read the image:\n",
    "# https://pillow.readthedocs.io/en/stable/handbook/concepts.html#concept-modes\n",
    "# with BGR being the one exception. One can set image format to BGR, we will\n",
    "# internally use RGB for conversion and flip the channels over\n",
    "_C.INPUT.FORMAT = \"BGR\"\n",
    "# The ground truth mask format that the model will use.\n",
    "# Mask R-CNN supports either \"polygon\" or \"bitmask\" as ground truth.\n",
    "_C.INPUT.MASK_FORMAT = \"polygon\"  # alternative: \"bitmask\"\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Dataset\n",
    "# -----------------------------------------------------------------------------\n",
    "_C.DATASETS = CN()\n",
    "# List of the dataset names for training. Must be registered in DatasetCatalog\n",
    "# Samples from these datasets will be merged and used as one dataset.\n",
    "_C.DATASETS.TRAIN = ()\n",
    "# List of the pre-computed proposal files for training, which must be consistent\n",
    "# with datasets listed in DATASETS.TRAIN.\n",
    "_C.DATASETS.PROPOSAL_FILES_TRAIN = ()\n",
    "# Number of top scoring precomputed proposals to keep for training\n",
    "_C.DATASETS.PRECOMPUTED_PROPOSAL_TOPK_TRAIN = 2000\n",
    "# List of the dataset names for testing. Must be registered in DatasetCatalog\n",
    "_C.DATASETS.TEST = ()\n",
    "# List of the pre-computed proposal files for test, which must be consistent\n",
    "# with datasets listed in DATASETS.TEST.\n",
    "_C.DATASETS.PROPOSAL_FILES_TEST = ()\n",
    "# Number of top scoring precomputed proposals to keep for test\n",
    "_C.DATASETS.PRECOMPUTED_PROPOSAL_TOPK_TEST = 1000\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# DataLoader\n",
    "# -----------------------------------------------------------------------------\n",
    "_C.DATALOADER = CN()\n",
    "# Number of data loading threads\n",
    "_C.DATALOADER.NUM_WORKERS = 4\n",
    "# If True, each batch should contain only images for which the aspect ratio\n",
    "# is compatible. This groups portrait images together, and landscape images\n",
    "# are not batched with portrait images.\n",
    "_C.DATALOADER.ASPECT_RATIO_GROUPING = True\n",
    "# Options: TrainingSampler, RepeatFactorTrainingSampler\n",
    "_C.DATALOADER.SAMPLER_TRAIN = \"TrainingSampler\"\n",
    "# Repeat threshold for RepeatFactorTrainingSampler\n",
    "_C.DATALOADER.REPEAT_THRESHOLD = 0.0\n",
    "# Tf True, when working on datasets that have instance annotations, the\n",
    "# training dataloader will filter out images without associated annotations\n",
    "_C.DATALOADER.FILTER_EMPTY_ANNOTATIONS = True\n",
    "\n",
    "# ---------------------------------------------------------------------------- #\n",
    "# Backbone options\n",
    "# ---------------------------------------------------------------------------- #\n",
    "_C.MODEL.BACKBONE = CN()\n",
    "\n",
    "_C.MODEL.BACKBONE.NAME = \"build_resnet_backbone\"\n",
    "# Freeze the first several stages so they are not trained.\n",
    "# There are 5 stages in ResNet. The first is a convolution, and the following\n",
    "# stages are each group of residual blocks.\n",
    "_C.MODEL.BACKBONE.FREEZE_AT = 2\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------- #\n",
    "# FPN options\n",
    "# ---------------------------------------------------------------------------- #\n",
    "_C.MODEL.FPN = CN()\n",
    "# Names of the input feature maps to be used by FPN\n",
    "# They must have contiguous power of 2 strides\n",
    "# e.g., [\"res2\", \"res3\", \"res4\", \"res5\"]\n",
    "_C.MODEL.FPN.IN_FEATURES = []\n",
    "_C.MODEL.FPN.OUT_CHANNELS = 256\n",
    "\n",
    "# Options: \"\" (no norm), \"GN\"\n",
    "_C.MODEL.FPN.NORM = \"\"\n",
    "\n",
    "# Types for fusing the FPN top-down and lateral features. Can be either \"sum\" or \"avg\"\n",
    "_C.MODEL.FPN.FUSE_TYPE = \"sum\"\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------- #\n",
    "# Proposal generator options\n",
    "# ---------------------------------------------------------------------------- #\n",
    "_C.MODEL.PROPOSAL_GENERATOR = CN()\n",
    "# Current proposal generators include \"RPN\", \"RRPN\" and \"PrecomputedProposals\"\n",
    "_C.MODEL.PROPOSAL_GENERATOR.NAME = \"RPN\"\n",
    "# Proposal height and width both need to be greater than MIN_SIZE\n",
    "# (a the scale used during training or inference)\n",
    "_C.MODEL.PROPOSAL_GENERATOR.MIN_SIZE = 0\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------- #\n",
    "# Anchor generator options\n",
    "# ---------------------------------------------------------------------------- #\n",
    "_C.MODEL.ANCHOR_GENERATOR = CN()\n",
    "# The generator can be any name in the ANCHOR_GENERATOR registry\n",
    "_C.MODEL.ANCHOR_GENERATOR.NAME = \"DefaultAnchorGenerator\"\n",
    "# Anchor sizes (i.e. sqrt of area) in absolute pixels w.r.t. the network input.\n",
    "# Format: list[list[float]]. SIZES[i] specifies the list of sizes to use for\n",
    "# IN_FEATURES[i]; len(SIZES) must be equal to len(IN_FEATURES) or 1.\n",
    "# When len(SIZES) == 1, SIZES[0] is used for all IN_FEATURES.\n",
    "_C.MODEL.ANCHOR_GENERATOR.SIZES = [[32, 64, 128, 256, 512]]\n",
    "# Anchor aspect ratios. For each area given in `SIZES`, anchors with different aspect\n",
    "# ratios are generated by an anchor generator.\n",
    "# Format: list[list[float]]. ASPECT_RATIOS[i] specifies the list of aspect ratios (H/W)\n",
    "# to use for IN_FEATURES[i]; len(ASPECT_RATIOS) == len(IN_FEATURES) must be true,\n",
    "# or len(ASPECT_RATIOS) == 1 is true and aspect ratio list ASPECT_RATIOS[0] is used\n",
    "# for all IN_FEATURES.\n",
    "_C.MODEL.ANCHOR_GENERATOR.ASPECT_RATIOS = [[0.5, 1.0, 2.0]]\n",
    "# Anchor angles.\n",
    "# list[list[float]], the angle in degrees, for each input feature map.\n",
    "# ANGLES[i] specifies the list of angles for IN_FEATURES[i].\n",
    "_C.MODEL.ANCHOR_GENERATOR.ANGLES = [[-90, 0, 90]]\n",
    "# Relative offset between the center of the first anchor and the top-left corner of the image\n",
    "# Value has to be in [0, 1). Recommend to use 0.5, which means half stride.\n",
    "# The value is not expected to affect model accuracy.\n",
    "_C.MODEL.ANCHOR_GENERATOR.OFFSET = 0.0\n",
    "\n",
    "# ---------------------------------------------------------------------------- #\n",
    "# RPN options\n",
    "# ---------------------------------------------------------------------------- #\n",
    "_C.MODEL.RPN = CN()\n",
    "_C.MODEL.RPN.HEAD_NAME = \"StandardRPNHead\"  # used by RPN_HEAD_REGISTRY\n",
    "\n",
    "# Names of the input feature maps to be used by RPN\n",
    "# e.g., [\"p2\", \"p3\", \"p4\", \"p5\", \"p6\"] for FPN\n",
    "_C.MODEL.RPN.IN_FEATURES = [\"res4\"]\n",
    "# Remove RPN anchors that go outside the image by BOUNDARY_THRESH pixels\n",
    "# Set to -1 or a large value, e.g. 100000, to disable pruning anchors\n",
    "_C.MODEL.RPN.BOUNDARY_THRESH = -1\n",
    "# IOU overlap ratios [BG_IOU_THRESHOLD, FG_IOU_THRESHOLD]\n",
    "# Minimum overlap required between an anchor and ground-truth box for the\n",
    "# (anchor, gt box) pair to be a positive example (IoU >= FG_IOU_THRESHOLD\n",
    "# ==> positive RPN example: 1)\n",
    "# Maximum overlap allowed between an anchor and ground-truth box for the\n",
    "# (anchor, gt box) pair to be a negative examples (IoU < BG_IOU_THRESHOLD\n",
    "# ==> negative RPN example: 0)\n",
    "# Anchors with overlap in between (BG_IOU_THRESHOLD <= IoU < FG_IOU_THRESHOLD)\n",
    "# are ignored (-1)\n",
    "_C.MODEL.RPN.IOU_THRESHOLDS = [0.3, 0.7]\n",
    "_C.MODEL.RPN.IOU_LABELS = [0, -1, 1]\n",
    "# Number of regions per image used to train RPN\n",
    "_C.MODEL.RPN.BATCH_SIZE_PER_IMAGE = 256\n",
    "# Target fraction of foreground (positive) examples per RPN minibatch\n",
    "_C.MODEL.RPN.POSITIVE_FRACTION = 0.5\n",
    "# Options are: \"smooth_l1\", \"giou\", \"diou\", \"ciou\"\n",
    "_C.MODEL.RPN.BBOX_REG_LOSS_TYPE = \"smooth_l1\"\n",
    "_C.MODEL.RPN.BBOX_REG_LOSS_WEIGHT = 1.0\n",
    "# Weights on (dx, dy, dw, dh) for normalizing RPN anchor regression targets\n",
    "_C.MODEL.RPN.BBOX_REG_WEIGHTS = (1.0, 1.0, 1.0, 1.0)\n",
    "# The transition point from L1 to L2 loss. Set to 0.0 to make the loss simply L1.\n",
    "_C.MODEL.RPN.SMOOTH_L1_BETA = 0.0\n",
    "_C.MODEL.RPN.LOSS_WEIGHT = 1.0\n",
    "# Number of top scoring RPN proposals to keep before applying NMS\n",
    "# When FPN is used, this is *per FPN level* (not total)\n",
    "_C.MODEL.RPN.PRE_NMS_TOPK_TRAIN = 12000\n",
    "_C.MODEL.RPN.PRE_NMS_TOPK_TEST = 6000\n",
    "# Number of top scoring RPN proposals to keep after applying NMS\n",
    "# When FPN is used, this limit is applied per level and then again to the union\n",
    "# of proposals from all levels\n",
    "# NOTE: When FPN is used, the meaning of this config is different from Detectron1.\n",
    "# It means per-batch topk in Detectron1, but per-image topk here.\n",
    "# See the \"find_top_rpn_proposals\" function for details.\n",
    "_C.MODEL.RPN.POST_NMS_TOPK_TRAIN = 2000\n",
    "_C.MODEL.RPN.POST_NMS_TOPK_TEST = 1000\n",
    "# NMS threshold used on RPN proposals\n",
    "_C.MODEL.RPN.NMS_THRESH = 0.7\n",
    "# Set this to -1 to use the same number of output channels as input channels.\n",
    "_C.MODEL.RPN.CONV_DIMS = [-1]\n",
    "\n",
    "# ---------------------------------------------------------------------------- #\n",
    "# ROI HEADS options\n",
    "# ---------------------------------------------------------------------------- #\n",
    "_C.MODEL.ROI_HEADS = CN()\n",
    "_C.MODEL.ROI_HEADS.NAME = \"Res5ROIHeads\"\n",
    "# Number of foreground classes\n",
    "_C.MODEL.ROI_HEADS.NUM_CLASSES = 80\n",
    "# Names of the input feature maps to be used by ROI heads\n",
    "# Currently all heads (box, mask, ...) use the same input feature map list\n",
    "# e.g., [\"p2\", \"p3\", \"p4\", \"p5\"] is commonly used for FPN\n",
    "_C.MODEL.ROI_HEADS.IN_FEATURES = [\"res4\"]\n",
    "# IOU overlap ratios [IOU_THRESHOLD]\n",
    "# Overlap threshold for an RoI to be considered background (if < IOU_THRESHOLD)\n",
    "# Overlap threshold for an RoI to be considered foreground (if >= IOU_THRESHOLD)\n",
    "_C.MODEL.ROI_HEADS.IOU_THRESHOLDS = [0.5]\n",
    "_C.MODEL.ROI_HEADS.IOU_LABELS = [0, 1]\n",
    "# RoI minibatch size *per image* (number of regions of interest [ROIs]) during training\n",
    "# Total number of RoIs per training minibatch =\n",
    "#   ROI_HEADS.BATCH_SIZE_PER_IMAGE * SOLVER.IMS_PER_BATCH\n",
    "# E.g., a common configuration is: 512 * 16 = 8192\n",
    "_C.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512\n",
    "# Target fraction of RoI minibatch that is labeled foreground (i.e. class > 0)\n",
    "_C.MODEL.ROI_HEADS.POSITIVE_FRACTION = 0.25\n",
    "\n",
    "# Only used on test mode\n",
    "\n",
    "# Minimum score threshold (assuming scores in a [0, 1] range); a value chosen to\n",
    "# balance obtaining high recall with not having too many low precision\n",
    "# detections that will slow down inference post processing steps (like NMS)\n",
    "# A default threshold of 0.0 increases AP by ~0.2-0.3 but significantly slows down\n",
    "# inference.\n",
    "_C.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.05\n",
    "# Overlap threshold used for non-maximum suppression (suppress boxes with\n",
    "# IoU >= this threshold)\n",
    "_C.MODEL.ROI_HEADS.NMS_THRESH_TEST = 0.5\n",
    "# If True, augment proposals with ground-truth boxes before sampling proposals to\n",
    "# train ROI heads.\n",
    "_C.MODEL.ROI_HEADS.PROPOSAL_APPEND_GT = True\n",
    "\n",
    "# ---------------------------------------------------------------------------- #\n",
    "# Box Head\n",
    "# ---------------------------------------------------------------------------- #\n",
    "_C.MODEL.ROI_BOX_HEAD = CN()\n",
    "# C4 don't use head name option\n",
    "# Options for non-C4 models: FastRCNNConvFCHead,\n",
    "_C.MODEL.ROI_BOX_HEAD.NAME = \"\"\n",
    "# Options are: \"smooth_l1\", \"giou\", \"diou\", \"ciou\"\n",
    "_C.MODEL.ROI_BOX_HEAD.BBOX_REG_LOSS_TYPE = \"smooth_l1\"\n",
    "# The final scaling coefficient on the box regression loss, used to balance the magnitude of its\n",
    "# gradients with other losses in the model. See also `MODEL.ROI_KEYPOINT_HEAD.LOSS_WEIGHT`.\n",
    "_C.MODEL.ROI_BOX_HEAD.BBOX_REG_LOSS_WEIGHT = 1.0\n",
    "# Default weights on (dx, dy, dw, dh) for normalizing bbox regression targets\n",
    "# These are empirically chosen to approximately lead to unit variance targets\n",
    "_C.MODEL.ROI_BOX_HEAD.BBOX_REG_WEIGHTS = (10.0, 10.0, 5.0, 5.0)\n",
    "# The transition point from L1 to L2 loss. Set to 0.0 to make the loss simply L1.\n",
    "_C.MODEL.ROI_BOX_HEAD.SMOOTH_L1_BETA = 0.0\n",
    "_C.MODEL.ROI_BOX_HEAD.POOLER_RESOLUTION = 14\n",
    "_C.MODEL.ROI_BOX_HEAD.POOLER_SAMPLING_RATIO = 0\n",
    "# Type of pooling operation applied to the incoming feature map for each RoI\n",
    "_C.MODEL.ROI_BOX_HEAD.POOLER_TYPE = \"ROIAlignV2\"\n",
    "\n",
    "_C.MODEL.ROI_BOX_HEAD.NUM_FC = 0\n",
    "# Hidden layer dimension for FC layers in the RoI box head\n",
    "_C.MODEL.ROI_BOX_HEAD.FC_DIM = 1024\n",
    "_C.MODEL.ROI_BOX_HEAD.NUM_CONV = 0\n",
    "# Channel dimension for Conv layers in the RoI box head\n",
    "_C.MODEL.ROI_BOX_HEAD.CONV_DIM = 256\n",
    "# Normalization method for the convolution layers.\n",
    "# Options: \"\" (no norm), \"GN\", \"SyncBN\".\n",
    "_C.MODEL.ROI_BOX_HEAD.NORM = \"\"\n",
    "# Whether to use class agnostic for bbox regression\n",
    "_C.MODEL.ROI_BOX_HEAD.CLS_AGNOSTIC_BBOX_REG = False\n",
    "# If true, RoI heads use bounding boxes predicted by the box head rather than proposal boxes.\n",
    "_C.MODEL.ROI_BOX_HEAD.TRAIN_ON_PRED_BOXES = False\n",
    "\n",
    "# Federated loss can be used to improve the training of LVIS\n",
    "_C.MODEL.ROI_BOX_HEAD.USE_FED_LOSS = False\n",
    "# Sigmoid cross entrophy is used with federated loss\n",
    "_C.MODEL.ROI_BOX_HEAD.USE_SIGMOID_CE = False\n",
    "# The power value applied to image_count when calcualting frequency weight\n",
    "_C.MODEL.ROI_BOX_HEAD.FED_LOSS_FREQ_WEIGHT_POWER = 0.5\n",
    "# Number of classes to keep in total\n",
    "_C.MODEL.ROI_BOX_HEAD.FED_LOSS_NUM_CLASSES = 50\n",
    "\n",
    "# ---------------------------------------------------------------------------- #\n",
    "# Cascaded Box Head\n",
    "# ---------------------------------------------------------------------------- #\n",
    "_C.MODEL.ROI_BOX_CASCADE_HEAD = CN()\n",
    "# The number of cascade stages is implicitly defined by the length of the following two configs.\n",
    "_C.MODEL.ROI_BOX_CASCADE_HEAD.BBOX_REG_WEIGHTS = (\n",
    "    (10.0, 10.0, 5.0, 5.0),\n",
    "    (20.0, 20.0, 10.0, 10.0),\n",
    "    (30.0, 30.0, 15.0, 15.0),\n",
    ")\n",
    "_C.MODEL.ROI_BOX_CASCADE_HEAD.IOUS = (0.5, 0.6, 0.7)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------- #\n",
    "# Mask Head\n",
    "# ---------------------------------------------------------------------------- #\n",
    "_C.MODEL.ROI_MASK_HEAD = CN()\n",
    "_C.MODEL.ROI_MASK_HEAD.NAME = \"MaskRCNNConvUpsampleHead\"\n",
    "_C.MODEL.ROI_MASK_HEAD.POOLER_RESOLUTION = 14\n",
    "_C.MODEL.ROI_MASK_HEAD.POOLER_SAMPLING_RATIO = 0\n",
    "_C.MODEL.ROI_MASK_HEAD.NUM_CONV = 0  # The number of convs in the mask head\n",
    "_C.MODEL.ROI_MASK_HEAD.CONV_DIM = 256\n",
    "# Normalization method for the convolution layers.\n",
    "# Options: \"\" (no norm), \"GN\", \"SyncBN\".\n",
    "_C.MODEL.ROI_MASK_HEAD.NORM = \"\"\n",
    "# Whether to use class agnostic for mask prediction\n",
    "_C.MODEL.ROI_MASK_HEAD.CLS_AGNOSTIC_MASK = False\n",
    "# Type of pooling operation applied to the incoming feature map for each RoI\n",
    "_C.MODEL.ROI_MASK_HEAD.POOLER_TYPE = \"ROIAlignV2\"\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------- #\n",
    "# Keypoint Head\n",
    "# ---------------------------------------------------------------------------- #\n",
    "_C.MODEL.ROI_KEYPOINT_HEAD = CN()\n",
    "_C.MODEL.ROI_KEYPOINT_HEAD.NAME = \"KRCNNConvDeconvUpsampleHead\"\n",
    "_C.MODEL.ROI_KEYPOINT_HEAD.POOLER_RESOLUTION = 14\n",
    "_C.MODEL.ROI_KEYPOINT_HEAD.POOLER_SAMPLING_RATIO = 0\n",
    "_C.MODEL.ROI_KEYPOINT_HEAD.CONV_DIMS = tuple(512 for _ in range(8))\n",
    "_C.MODEL.ROI_KEYPOINT_HEAD.NUM_KEYPOINTS = 17  # 17 is the number of keypoints in COCO.\n",
    "\n",
    "# Images with too few (or no) keypoints are excluded from training.\n",
    "_C.MODEL.ROI_KEYPOINT_HEAD.MIN_KEYPOINTS_PER_IMAGE = 1\n",
    "# Normalize by the total number of visible keypoints in the minibatch if True.\n",
    "# Otherwise, normalize by the total number of keypoints that could ever exist\n",
    "# in the minibatch.\n",
    "# The keypoint softmax loss is only calculated on visible keypoints.\n",
    "# Since the number of visible keypoints can vary significantly between\n",
    "# minibatches, this has the effect of up-weighting the importance of\n",
    "# minibatches with few visible keypoints. (Imagine the extreme case of\n",
    "# only one visible keypoint versus N: in the case of N, each one\n",
    "# contributes 1/N to the gradient compared to the single keypoint\n",
    "# determining the gradient direction). Instead, we can normalize the\n",
    "# loss by the total number of keypoints, if it were the case that all\n",
    "# keypoints were visible in a full minibatch. (Returning to the example,\n",
    "# this means that the one visible keypoint contributes as much as each\n",
    "# of the N keypoints.)\n",
    "_C.MODEL.ROI_KEYPOINT_HEAD.NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS = True\n",
    "# Multi-task loss weight to use for keypoints\n",
    "# Recommended values:\n",
    "#   - use 1.0 if NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS is True\n",
    "#   - use 4.0 if NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS is False\n",
    "_C.MODEL.ROI_KEYPOINT_HEAD.LOSS_WEIGHT = 1.0\n",
    "# Type of pooling operation applied to the incoming feature map for each RoI\n",
    "_C.MODEL.ROI_KEYPOINT_HEAD.POOLER_TYPE = \"ROIAlignV2\"\n",
    "\n",
    "# ---------------------------------------------------------------------------- #\n",
    "# Semantic Segmentation Head\n",
    "# ---------------------------------------------------------------------------- #\n",
    "_C.MODEL.SEM_SEG_HEAD = CN()\n",
    "_C.MODEL.SEM_SEG_HEAD.NAME = \"SemSegFPNHead\"\n",
    "_C.MODEL.SEM_SEG_HEAD.IN_FEATURES = [\"p2\", \"p3\", \"p4\", \"p5\"]\n",
    "# Label in the semantic segmentation ground truth that is ignored, i.e., no loss is calculated for\n",
    "# the correposnding pixel.\n",
    "_C.MODEL.SEM_SEG_HEAD.IGNORE_VALUE = 255\n",
    "# Number of classes in the semantic segmentation head\n",
    "_C.MODEL.SEM_SEG_HEAD.NUM_CLASSES = 54\n",
    "# Number of channels in the 3x3 convs inside semantic-FPN heads.\n",
    "_C.MODEL.SEM_SEG_HEAD.CONVS_DIM = 128\n",
    "# Outputs from semantic-FPN heads are up-scaled to the COMMON_STRIDE stride.\n",
    "_C.MODEL.SEM_SEG_HEAD.COMMON_STRIDE = 4\n",
    "# Normalization method for the convolution layers. Options: \"\" (no norm), \"GN\".\n",
    "_C.MODEL.SEM_SEG_HEAD.NORM = \"GN\"\n",
    "_C.MODEL.SEM_SEG_HEAD.LOSS_WEIGHT = 1.0\n",
    "\n",
    "_C.MODEL.PANOPTIC_FPN = CN()\n",
    "# Scaling of all losses from instance detection / segmentation head.\n",
    "_C.MODEL.PANOPTIC_FPN.INSTANCE_LOSS_WEIGHT = 1.0\n",
    "\n",
    "# options when combining instance & semantic segmentation outputs\n",
    "_C.MODEL.PANOPTIC_FPN.COMBINE = CN({\"ENABLED\": True})  # \"COMBINE.ENABLED\" is deprecated & not used\n",
    "_C.MODEL.PANOPTIC_FPN.COMBINE.OVERLAP_THRESH = 0.5\n",
    "_C.MODEL.PANOPTIC_FPN.COMBINE.STUFF_AREA_LIMIT = 4096\n",
    "_C.MODEL.PANOPTIC_FPN.COMBINE.INSTANCES_CONFIDENCE_THRESH = 0.5\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------- #\n",
    "# RetinaNet Head\n",
    "# ---------------------------------------------------------------------------- #\n",
    "_C.MODEL.RETINANET = CN()\n",
    "\n",
    "# This is the number of foreground classes.\n",
    "_C.MODEL.RETINANET.NUM_CLASSES = 80\n",
    "\n",
    "_C.MODEL.RETINANET.IN_FEATURES = [\"p3\", \"p4\", \"p5\", \"p6\", \"p7\"]\n",
    "\n",
    "# Convolutions to use in the cls and bbox tower\n",
    "# NOTE: this doesn't include the last conv for logits\n",
    "_C.MODEL.RETINANET.NUM_CONVS = 4\n",
    "\n",
    "# IoU overlap ratio [bg, fg] for labeling anchors.\n",
    "# Anchors with < bg are labeled negative (0)\n",
    "# Anchors  with >= bg and < fg are ignored (-1)\n",
    "# Anchors with >= fg are labeled positive (1)\n",
    "_C.MODEL.RETINANET.IOU_THRESHOLDS = [0.4, 0.5]\n",
    "_C.MODEL.RETINANET.IOU_LABELS = [0, -1, 1]\n",
    "\n",
    "# Prior prob for rare case (i.e. foreground) at the beginning of training.\n",
    "# This is used to set the bias for the logits layer of the classifier subnet.\n",
    "# This improves training stability in the case of heavy class imbalance.\n",
    "_C.MODEL.RETINANET.PRIOR_PROB = 0.01\n",
    "\n",
    "# Inference cls score threshold, only anchors with score > INFERENCE_TH are\n",
    "# considered for inference (to improve speed)\n",
    "_C.MODEL.RETINANET.SCORE_THRESH_TEST = 0.05\n",
    "# Select topk candidates before NMS\n",
    "_C.MODEL.RETINANET.TOPK_CANDIDATES_TEST = 1000\n",
    "_C.MODEL.RETINANET.NMS_THRESH_TEST = 0.5\n",
    "\n",
    "# Weights on (dx, dy, dw, dh) for normalizing Retinanet anchor regression targets\n",
    "_C.MODEL.RETINANET.BBOX_REG_WEIGHTS = (1.0, 1.0, 1.0, 1.0)\n",
    "\n",
    "# Loss parameters\n",
    "_C.MODEL.RETINANET.FOCAL_LOSS_GAMMA = 2.0\n",
    "_C.MODEL.RETINANET.FOCAL_LOSS_ALPHA = 0.25\n",
    "_C.MODEL.RETINANET.SMOOTH_L1_LOSS_BETA = 0.1\n",
    "# Options are: \"smooth_l1\", \"giou\", \"diou\", \"ciou\"\n",
    "_C.MODEL.RETINANET.BBOX_REG_LOSS_TYPE = \"smooth_l1\"\n",
    "\n",
    "# One of BN, SyncBN, FrozenBN, GN\n",
    "# Only supports GN until unshared norm is implemented\n",
    "_C.MODEL.RETINANET.NORM = \"\"\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------- #\n",
    "# ResNe[X]t options (ResNets = {ResNet, ResNeXt}\n",
    "# Note that parts of a resnet may be used for both the backbone and the head\n",
    "# These options apply to both\n",
    "# ---------------------------------------------------------------------------- #\n",
    "_C.MODEL.RESNETS = CN()\n",
    "\n",
    "_C.MODEL.RESNETS.DEPTH = 50\n",
    "_C.MODEL.RESNETS.OUT_FEATURES = [\"res4\"]  # res4 for C4 backbone, res2..5 for FPN backbone\n",
    "\n",
    "# Number of groups to use; 1 ==> ResNet; > 1 ==> ResNeXt\n",
    "_C.MODEL.RESNETS.NUM_GROUPS = 1\n",
    "\n",
    "# Options: FrozenBN, GN, \"SyncBN\", \"BN\"\n",
    "_C.MODEL.RESNETS.NORM = \"FrozenBN\"\n",
    "\n",
    "# Baseline width of each group.\n",
    "# Scaling this parameters will scale the width of all bottleneck layers.\n",
    "_C.MODEL.RESNETS.WIDTH_PER_GROUP = 64\n",
    "\n",
    "# Place the stride 2 conv on the 1x1 filter\n",
    "# Use True only for the original MSRA ResNet; use False for C2 and Torch models\n",
    "_C.MODEL.RESNETS.STRIDE_IN_1X1 = True\n",
    "\n",
    "# Apply dilation in stage \"res5\"\n",
    "_C.MODEL.RESNETS.RES5_DILATION = 1\n",
    "\n",
    "# Output width of res2. Scaling this parameters will scale the width of all 1x1 convs in ResNet\n",
    "# For R18 and R34, this needs to be set to 64\n",
    "_C.MODEL.RESNETS.RES2_OUT_CHANNELS = 256\n",
    "_C.MODEL.RESNETS.STEM_OUT_CHANNELS = 64\n",
    "\n",
    "# Apply Deformable Convolution in stages\n",
    "# Specify if apply deform_conv on Res2, Res3, Res4, Res5\n",
    "_C.MODEL.RESNETS.DEFORM_ON_PER_STAGE = [False, False, False, False]\n",
    "# Use True to use modulated deform_conv (DeformableV2, https://arxiv.org/abs/1811.11168);\n",
    "# Use False for DeformableV1.\n",
    "_C.MODEL.RESNETS.DEFORM_MODULATED = False\n",
    "# Number of groups in deformable conv.\n",
    "_C.MODEL.RESNETS.DEFORM_NUM_GROUPS = 1\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------- #\n",
    "# Solver\n",
    "# ---------------------------------------------------------------------------- #\n",
    "_C.SOLVER = CN()\n",
    "\n",
    "# Options: WarmupMultiStepLR, WarmupCosineLR.\n",
    "# See detectron2/solver/build.py for definition.\n",
    "_C.SOLVER.LR_SCHEDULER_NAME = \"WarmupMultiStepLR\"\n",
    "\n",
    "_C.SOLVER.MAX_ITER = 40000\n",
    "\n",
    "_C.SOLVER.BASE_LR = 0.001\n",
    "# The end lr, only used by WarmupCosineLR\n",
    "_C.SOLVER.BASE_LR_END = 0.0\n",
    "\n",
    "_C.SOLVER.MOMENTUM = 0.9\n",
    "\n",
    "_C.SOLVER.NESTEROV = False\n",
    "\n",
    "_C.SOLVER.WEIGHT_DECAY = 0.0001\n",
    "# The weight decay that's applied to parameters of normalization layers\n",
    "# (typically the affine transformation)\n",
    "_C.SOLVER.WEIGHT_DECAY_NORM = 0.0\n",
    "\n",
    "_C.SOLVER.GAMMA = 0.1\n",
    "# The iteration number to decrease learning rate by GAMMA.\n",
    "_C.SOLVER.STEPS = (30000,)\n",
    "\n",
    "_C.SOLVER.WARMUP_FACTOR = 1.0 / 1000\n",
    "_C.SOLVER.WARMUP_ITERS = 1000\n",
    "_C.SOLVER.WARMUP_METHOD = \"linear\"\n",
    "\n",
    "# Save a checkpoint after every this number of iterations\n",
    "_C.SOLVER.CHECKPOINT_PERIOD = 5000\n",
    "\n",
    "# Number of images per batch across all machines. This is also the number\n",
    "# of training images per step (i.e. per iteration). If we use 16 GPUs\n",
    "# and IMS_PER_BATCH = 32, each GPU will see 2 images per batch.\n",
    "# May be adjusted automatically if REFERENCE_WORLD_SIZE is set.\n",
    "_C.SOLVER.IMS_PER_BATCH = 16\n",
    "\n",
    "# The reference number of workers (GPUs) this config is meant to train with.\n",
    "# It takes no effect when set to 0.\n",
    "# With a non-zero value, it will be used by DefaultTrainer to compute a desired\n",
    "# per-worker batch size, and then scale the other related configs (total batch size,\n",
    "# learning rate, etc) to match the per-worker batch size.\n",
    "# See documentation of `DefaultTrainer.auto_scale_workers` for details:\n",
    "_C.SOLVER.REFERENCE_WORLD_SIZE = 0\n",
    "\n",
    "# Detectron v1 (and previous detection code) used a 2x higher LR and 0 WD for\n",
    "# biases. This is not useful (at least for recent models). You should avoid\n",
    "# changing these and they exist only to reproduce Detectron v1 training if\n",
    "# desired.\n",
    "_C.SOLVER.BIAS_LR_FACTOR = 1.0\n",
    "_C.SOLVER.WEIGHT_DECAY_BIAS = None  # None means following WEIGHT_DECAY\n",
    "\n",
    "# Gradient clipping\n",
    "_C.SOLVER.CLIP_GRADIENTS = CN({\"ENABLED\": False})\n",
    "# Type of gradient clipping, currently 2 values are supported:\n",
    "# - \"value\": the absolute values of elements of each gradients are clipped\n",
    "# - \"norm\": the norm of the gradient for each parameter is clipped thus\n",
    "#   affecting all elements in the parameter\n",
    "_C.SOLVER.CLIP_GRADIENTS.CLIP_TYPE = \"value\"\n",
    "# Maximum absolute value used for clipping gradients\n",
    "_C.SOLVER.CLIP_GRADIENTS.CLIP_VALUE = 1.0\n",
    "# Floating point number p for L-p norm to be used with the \"norm\"\n",
    "# gradient clipping type; for L-inf, please specify .inf\n",
    "_C.SOLVER.CLIP_GRADIENTS.NORM_TYPE = 2.0\n",
    "\n",
    "# Enable automatic mixed precision for training\n",
    "# Note that this does not change model's inference behavior.\n",
    "# To use AMP in inference, run inference under autocast()\n",
    "_C.SOLVER.AMP = CN({\"ENABLED\": False})\n",
    "\n",
    "# ---------------------------------------------------------------------------- #\n",
    "# Specific test options\n",
    "# ---------------------------------------------------------------------------- #\n",
    "_C.TEST = CN()\n",
    "# For end-to-end tests to verify the expected accuracy.\n",
    "# Each item is [task, metric, value, tolerance]\n",
    "# e.g.: [['bbox', 'AP', 38.5, 0.2]]\n",
    "_C.TEST.EXPECTED_RESULTS = []\n",
    "# The period (in terms of steps) to evaluate the model during training.\n",
    "# Set to 0 to disable.\n",
    "_C.TEST.EVAL_PERIOD = 0\n",
    "# The sigmas used to calculate keypoint OKS. See http://cocodataset.org/#keypoints-eval\n",
    "# When empty, it will use the defaults in COCO.\n",
    "# Otherwise it should be a list[float] with the same length as ROI_KEYPOINT_HEAD.NUM_KEYPOINTS.\n",
    "_C.TEST.KEYPOINT_OKS_SIGMAS = []\n",
    "# Maximum number of detections to return per image during inference (100 is\n",
    "# based on the limit established for the COCO dataset).\n",
    "_C.TEST.DETECTIONS_PER_IMAGE = 100\n",
    "\n",
    "_C.TEST.AUG = CN({\"ENABLED\": False})\n",
    "_C.TEST.AUG.MIN_SIZES = (400, 500, 600, 700, 800, 900, 1000, 1100, 1200)\n",
    "_C.TEST.AUG.MAX_SIZE = 4000\n",
    "_C.TEST.AUG.FLIP = True\n",
    "\n",
    "_C.TEST.PRECISE_BN = CN({\"ENABLED\": False})\n",
    "_C.TEST.PRECISE_BN.NUM_ITER = 200\n",
    "\n",
    "# ---------------------------------------------------------------------------- #\n",
    "# Misc options\n",
    "# ---------------------------------------------------------------------------- #\n",
    "# Directory where output files are written\n",
    "_C.OUTPUT_DIR = \"./output\"\n",
    "# Set seed to negative to fully randomize everything.\n",
    "# Set seed to positive to use a fixed seed. Note that a fixed seed increases\n",
    "# reproducibility but does not guarantee fully deterministic behavior.\n",
    "# Disabling all parallelism further increases reproducibility.\n",
    "_C.SEED = -1\n",
    "# Benchmark different cudnn algorithms.\n",
    "# If input images have very different sizes, this option will have large overhead\n",
    "# for about 10k iterations. It usually hurts total time, but can benefit for certain models.\n",
    "# If input images have the same or similar sizes, benchmark is often helpful.\n",
    "_C.CUDNN_BENCHMARK = False\n",
    "# The period (in terms of steps) for minibatch visualization at train time.\n",
    "# Set to 0 to disable.\n",
    "_C.VIS_PERIOD = 0\n",
    "\n",
    "# global config is for quick hack purposes.\n",
    "# You can set them in command line or config files,\n",
    "# and access it with:\n",
    "#\n",
    "# from detectron2.config import global_cfg\n",
    "# print(global_cfg.HACK)\n",
    "#\n",
    "# Do not commit any configs into it.\n",
    "_C.GLOBAL = CN()\n",
    "_C.GLOBAL.HACK = 1.0\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Train Detectron2 on custom dataset.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
